# Explain it!
---
# CoT-Caption: Studying Chain-of-Thought Structures in Vision-Language Models

## ğŸ“Œ Project Overview

This project investigates how **Chain-of-Thought (CoT) reasoning** affects the performance and behavior of modern vision-language models (LLVMs) in image captioning tasks. Rather than training new models, this is a **comparative case study** analyzing how different CoT prompting strategies impact caption **accuracy**, **uncertainty calibration**, and **perceived trustworthiness**.

## ğŸ¯ Research Objectives

We aim to answer three focused questions:

1. **Structure Effect**  
   Which CoT structure (e.g., stepwise, cause-effect, explanation-first) yields the most effective model performance?

2. **Uncertainty Calibration**  
   Does CoT help models express uncertainty in a more appropriate, calibrated way?

3. **Accuracy vs. Persuasiveness**  
   Does CoT improve factual correctness â€” or just make the model *sound* more confident and convincing?

---

## ğŸ§ª Methodology Summary

- **50 test images** shared across all experiments  
- **3 CoT structures** (prompt templates) per image  
- **3 models**: BLIP-2, DeepSeek-VL, GPT-4V  
- Each [image Ã— prompt] pair = one caption + CoT output  
- Total: **150 outputs/model Ã— 3 models = 450 outputs**
- Evaluation via automated scoring + light human judgment

---

## âš™ï¸ Models Used

| Model | Type | Notes |
|-------|------|-------|
| `BLIP-2` (`Salesforce/blip2-flan-t5-xl`) | Open-source | Controlled captioning baseline |
| `DeepSeek-VL` | Open-source | Chat-style multimodal reasoning |
| `GPT-4V` | Proprietary | High-quality oracle for contrast |

---

## â³ Milestones

| # | Task | Description |
|--|------|-------------|
| 1 | Project Setup | Finalize objectives, scope, and evaluation criteria |
| 2 | Define Prompt Structures & Models | Write CoT prompts and choose inference models |
| 3 | Create Test Set | Curate 50 diverse image cases and define prompt mappings |
| 4 | Run Inference | Generate model outputs for all image Ã— prompt pairs |
| 5 | Score Outputs | Compute CLIP Score, BERTScore, and uncertainty features |
| 6 | Analyze & Report | Summarize findings in final LaTeX report and repo |

---

## ğŸ› ï¸ Evaluation Metrics

| Metric | Purpose |
|--------|---------|
| **CLIP Score** | Caption-image alignment |
| **BERTScore** | Caption-reference semantic overlap |
| **Uncertainty Expression Tags** | Track hedging (â€œlikelyâ€, â€œprobablyâ€) |
| **Human Preference (optional)** | A/B testing for trustworthiness and clarity |

---

# ğŸ§  Multimodal CoT Evaluation â€” Project Tracker

---

## âœ… Setup & Scope Definition

- âœ… Finalized research question & model list (GPT-4V, DeepSeek-VL, BLIP-2)
- âœ… Chose 3 CoT prompt formats (Stepwise, Visual-first, Contrastive)
- âœ… Defined evaluation framework (Accuracy, Reasoning, Confidence)
- âœ… Decided on 50 total images, reused for all formats
- âœ… Dropped Modular and Meta from experiment scope
- âœ… Finalized label system (2 = Good, 1 = Partial, 0 = Bad)
- âœ… Decided on manual dual annotation + Cohenâ€™s Kappa

---

## ğŸ“… This Weekâ€™s Goals

### ğŸ“… April 22 (Tue)
- ğŸ“… Start image picking (no pressure to finish)

### ğŸ“… April 23 (Wed)
- ğŸ“… Finalize 50 image selection
- ğŸ“… Create 25 contrastive pairs from 50 images
- ğŸ“… Write 50 Stepwise prompts
- ğŸ“… Write 50 Visual-first prompts
- ğŸ“… Write 25 Contrastive prompts
- ğŸ“… Build `test_cases.csv` with all [image Ã— prompt structure] entries

### ğŸ“… April 24 (Thu)
- ğŸ“… Deploy and test GPT-4V
- ğŸ“… Deploy and test DeepSeek-VL
- ğŸ“… Deploy and test BLIP-2
- ğŸ“… Sync with Xinyu (first full project discussion)

### ğŸ“… April 25 (Fri)
- ğŸ“… Begin running prompts across all models
- ğŸ“… Save outputs to `generated_outputs/`
- ğŸ“… Begin logging progress into `progress_log.md`

---

## â¬œ Next Week & Beyond

### â¬œ Evaluation Phase
- â¬œ Create scoring templates (per rater)
- â¬œ Annotate all 375 outputs (2 raters)
- â¬œ Calculate Cohenâ€™s Kappa per dimension
- â¬œ Merge and finalize final `eval_results.csv`

### â¬œ Analysis & Writeup
- â¬œ Aggregate accuracy + reasoning scores
- â¬œ Select strong/weak example outputs
- â¬œ Create bar/line/pie charts per structure & model
- â¬œ Draft results summary + discussion insights

### â¬œ Final Report Polish
- â¬œ Insert results into report
- â¬œ Write final conclusion
- â¬œ Add appendix: prompts, eval rubric, sample outputs

### â¬œ Presentation Prep
- â¬œ Create slide deck
- â¬œ Design per-structure summary visuals
- â¬œ Rehearse walkthrough with Xinyu

