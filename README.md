# ğŸ§ª Chain-of-Thought Prompting for Chemistry Reasoning in VLMs

This project investigates how different Chain-of-Thought (CoT) prompting structures affect the reasoning abilities of state-of-the-art vision-language models (VLMs) on a suite of chemistry-focused image-based tasks.

---

## ğŸ“Œ Project Objectives

We systematically evaluate:

- **Model performance across VLMs**, including **LLaVA-OneVision**, **LLaVA-Med (Microsoft)**, and **DeepSeek-VL**, using curated molecule diagrams as input.
- **Prompt structure effectiveness** by comparing three CoT formatsâ€”**Stepwise**, **Visual-first**, and **Explanation-first**â€”against a **baseline (no CoT)** format.
- **Task-specific performance** across five types of chemistry reasoning, from visual recognition to mechanistic inference.
- **Output accuracy and reasoning quality** using a human-rated rubric (0 = incorrect, 1 = partial, 2 = correct), allowing consistent scoring across prompts, tasks, and models.
- **Trends and failure modes**, identifying which CoT formats and models produce reliable scientific reasoning and which fall back on heuristics or hallucinations.

---

## ğŸ“¦ Deliverables

- **âœ… Prompt Set**  
  A curated collection of prompts (baseline and CoT) for five chemistry tasks.

- **âœ… Inference Results**  
  Model outputs for each prompt-task-model combination, stored as structured `.csv` files.

- **âœ… Graded Dataset**  
  Human-annotated scores (0â€“2) for all model outputs, including qualitative notes on reasoning.

- **âœ… Data Analysis Report**  
  - Accuracy comparisons across models and prompt types  
  - Identification of effective prompt structures  
  - Observations on task complexity and model robustness

- **âœ… Findings and Recommendations**  
  Summary discussion on:
  - How CoT prompting influences multimodal scientific reasoning
  - Common reasoning failures and speculation patterns
  - Implications for future prompt design in scientific domains

---

## âš™ï¸ Setup Overview

### ğŸ” Models Evaluated
- `LLaVA-OneVision`  
- `LLaVA-Med` (v1.5 Mistral, biomedical)  
- `DeepSeek-VL` (multilingual vision-language model)

### ğŸ§ª Tasks (for each model)
| Task ID | Task Description |
|---------|------------------|
| Task 0  | Molecule identification |
| Task 1  | EAS (Electrophilic Aromatic Substitution) reactivity |
| Task 2  | Acid/Base strength comparison |
| Task 3  | Nucleophilicity (functional group reactivity) |
| Task 4  | SN1 reaction likelihood |

### ğŸ’¬ Prompt Structures
- `Baseline` (no CoT)
- `Stepwise`
- `Visual-first`
- `Explanation-first`

### ğŸ§® Scoring Rubric
| Score | Meaning |
|-------|---------|
| 0     | Incorrect or off-topic answer |
| 1     | Partially correct, incomplete or shallow reasoning |
| 2     | Fully correct with sound reasoning |

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€Analysis/      # store all result evaluations & analysis guideline doc
â”‚   â”œâ”€â”€ Analysis_Design_Doc.md
â”‚   â”œâ”€â”€ Task0_1V_EvalRes.md
â”‚   â”œâ”€â”€ Task1_EvalRes.md
â”‚   â”œâ”€â”€ Task2_EvalRes.md
â”‚   â”œâ”€â”€ Task3_EvalRes.md
â”‚   â””â”€â”€ Task4_EvalRes.md
â”œâ”€â”€ Dataset/      # stores all images in .png
â”‚   â”œâ”€â”€ All
â”‚   â”œâ”€â”€ Task0
â”‚   â”œâ”€â”€ Task1
â”‚   â”œâ”€â”€ Task2
â”‚   â”œâ”€â”€ Task3
â”‚   â””â”€â”€ Task4
â”œâ”€â”€ Results/      # stores all results in .csv/.txt/xlsx
â”‚   â”œâ”€â”€ Task0
â”‚   â”œâ”€â”€ Task1
â”‚   â”œâ”€â”€ Task2
â”‚   â”œâ”€â”€ Task3
â”‚   â””â”€â”€ Task4
â”œâ”€â”€ Scripts/      # stores all .py code scripts for finetuning prompts and running tasks on models
â”‚   â”œâ”€â”€ Task0
â”‚   â”œâ”€â”€ Task1
â”‚   â”œâ”€â”€ Task2
â”‚   â”œâ”€â”€ Task3
â”‚   â””â”€â”€ Task4
â”œâ”€â”€ Task_Definitions/    # what each task is about, images used, prompts selected
â”‚   â”œâ”€â”€ 0_Identify.md
â”‚   â”œâ”€â”€ 1_EAS.md
â”‚   â”œâ”€â”€ 2_AcidBase.md
â”‚   â”œâ”€â”€ 3_FunctionalGroups.md
â”‚   â”œâ”€â”€ 4_SN1SN2.md
â”‚   â”œâ”€â”€ meta.csv
â”‚   â””â”€â”€ pair_list.csv
â”œâ”€â”€ Img/       # figures
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
