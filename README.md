# ğŸ§  CoT-Caption: Studying Chain-of-Thought Structures in Vision-Language Models

---

## ğŸ“Œ Project Overview

This project investigates how **Chain-of-Thought (CoT) prompting structures** affect the reasoning behavior of vision-language models (VLMs) in image captioning tasks.  
Rather than modifying models or datasets, we perform a **comparative case study** using prompt-only methods to test how different CoT styles influence:

- Caption **accuracy**
- Expression of **uncertainty**
- Perceived **persuasiveness or trustworthiness**

---

## ğŸ¯ Research Objectives

We aim to answer three focused questions:

1. **Structure Effect**  
   Which CoT structure (e.g., stepwise, visual-first, contrastive) yields the most effective model performance?

2. **Uncertainty Calibration**  
   Does CoT help models express uncertainty in a more appropriate, calibrated way?

3. **Accuracy vs. Persuasiveness**  
   Does CoT improve factual correctness â€” or just make the model *sound* more confident and convincing?

---

## ğŸ§ª Methodology Summary

- **50 total images**, reused across all prompt formats
- **4 prompt structures** per image:
  - Stepwise (explicit â€œLetâ€™s think step by stepâ€)
  - Visual-first (starts with scene observation)
  - Contrastive (image pairs)
  - Baseline (no CoT guidance)
- **3 models**:
  - `GPT-4V`
  - `DeepSeek-VL`
  - `BLIP-2`
- **175 prompts total Ã— 3 models = 525 outputs**
- Prompting is performed in **stateless, one-shot sessions**
- Outputs are evaluated using **manual dual annotation**

---

## âš™ï¸ Models Used

| Model | Type | Notes |
|-------|------|-------|
| `BLIP-2` (`Salesforce/blip2-flan-t5-xl`) | Open-source | Controlled baseline model |
| `DeepSeek-VL` | Open-source | Chat-style VLM |
| `GPT-4V` | Proprietary | Oracle-quality multimodal reasoning |

---

## ğŸ› ï¸ Evaluation Metrics

| Metric | Description |
|--------|-------------|
| âœ… **Accuracy** (0â€“2) | Final answer correctness |
| âœ… **Reasoning Quality** (0â€“2) | Step coherence, justification |
| âœ… **Confidence Expression** (0â€“2) | Explicit uncertainty vs overconfidence |
| âœ… **Persuasiveness** (0â€“2) | How convincing the output sounds, regardless of correctness |
| âœ… **Cohenâ€™s Kappa** | Inter-rater agreement for all scores |
| ğŸŸ¡ *(Optional)* CLIPScore, BERTScore | Caption similarity metrics (TBD) |

All evaluations are logged in a shared scoring sheet with matching image IDs and prompt types.

---

## ğŸ§ª Annotation Procedure

- Two annotators independently label each model output
- Labels are:
  - Accuracy: `Incorrect (0)`, `Partial (1)`, `Correct (2)`
  - Reasoning Quality: `Hallucinated`, `Incomplete`, `Clear & Justified`
  - Confidence: `Explicitly Uncertain`, `Hedged`, `Confident`
  - Persuasiveness: `Unconvincing`, `Somewhat convincing`, `Very persuasive`
- Final scores are merged and agreement is measured using **Cohenâ€™s Kappa**

---

## â³ Milestones

| # | Task | Description |
|--|------|-------------|
| 1 | Project Setup | Define objectives and evaluation plan |
| 2 | Prompt Structure Design | Write 4 prompt types (incl. baseline) |
| 3 | Image Set Creation | Curate 50 diverse images and pairings |
| 4 | Model Execution | Run all models across 175 prompts |
| 5 | Manual Annotation | Score 525 outputs across 4 dimensions |
| 6 | Analysis & Reporting | Aggregate findings, finalize report |

---

# ğŸš¦ Multimodal CoT Evaluation â€” Project Tracker

## âœ… Setup & Scope

- âœ… Finalized research question and model list
- âœ… Selected 3 CoT structures + 1 baseline
- âœ… Evaluation framework: accuracy, reasoning, confidence, persuasiveness
- âœ… Dual annotation setup + Cohenâ€™s Kappa
- âœ… All known biases (randomness, meta exposure) acknowledged

---

## ğŸ“… This Weekâ€™s Plan

| Date | Task |
|------|------|
| Apr 22 | Start image picking (no pressure to finish) |
| Apr 23 | Finalize 50 images and 25 contrastive pairs; Write 150 CoT prompts |
| Apr 24 | Deploy + test all 3 models; sync with Xinyu |
| Apr 25 | Start running all prompts across models and logging outputs |

---

## ğŸ”œ Next Week & Beyond

### â¬œ Evaluation
- â¬œ Create scoring templates for both annotators
- â¬œ Annotate all 525 outputs (manual)
- â¬œ Calculate Cohenâ€™s Kappa for each score type

### â¬œ Analysis
- â¬œ Compute accuracy vs confidence patterns
- â¬œ Analyze persuasiveness vs correctness
- â¬œ Extract standout examples + trends
- â¬œ Create visualizations (bar/pie/line charts)

### â¬œ Report & Presentation
- â¬œ Finalize LaTeX report
- â¬œ Add appendix: image list, rubric, prompt examples
- â¬œ Build slide deck for final presentation
