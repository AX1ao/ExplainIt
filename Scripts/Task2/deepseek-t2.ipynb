{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnuIwZedwjhZ",
        "outputId": "56fe3a6f-dc1f-416f-8141-45d6f7283ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-11 05:40:26--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.191.158, 104.16.32.241, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.191.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155472915 (148M) [application/octet-stream]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>] 148.27M   216MB/s    in 0.7s    \n",
            "\n",
            "2025-05-11 05:40:27 (216 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [155472915/155472915]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/py38\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    pip-24.2                   |   py38h06a4308_0         2.2 MB\n",
            "    python-3.8.20              |       he870216_0        23.8 MB\n",
            "    setuptools-75.1.0          |   py38h06a4308_0         1.7 MB\n",
            "    wheel-0.44.0               |   py38h06a4308_0         108 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0 \n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0 \n",
            "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
            "  openssl            pkgs/main/linux-64::openssl-3.0.16-h5eee18b_0 \n",
            "  pip                pkgs/main/linux-64::pip-24.2-py38h06a4308_0 \n",
            "  python             pkgs/main/linux-64::python-3.8.20-he870216_0 \n",
            "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-75.1.0-py38h06a4308_0 \n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0 \n",
            "  tk                 pkgs/main/linux-64::tk-8.6.14-h39e8969_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.44.0-py38h06a4308_0 \n",
            "  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1 \n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.8.20        | 23.8 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "pip-24.2             | 2.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "setuptools-75.1.0    | 1.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "wheel-0.44.0         | 108 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "python-3.8.20        | 23.8 MB   | :   3% 0.03220284015527493/1 [00:00<00:03,  3.11s/it]\n",
            "pip-24.2             | 2.2 MB    | :  18% 0.1839029646616749/1 [00:00<00:00,  1.83it/s]\u001b[A\n",
            "\n",
            "setuptools-75.1.0    | 1.7 MB    | :  11% 0.11099105446243789/1 [00:00<00:00,  1.09it/s]\u001b[A\u001b[A\n",
            "pip-24.2             | 2.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.83it/s]               \u001b[A\n",
            "\n",
            "\n",
            "wheel-0.44.0         | 108 KB    | : 100% 1.0/1 [00:00<00:00,  6.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "wheel-0.44.0         | 108 KB    | : 100% 1.0/1 [00:00<00:00,  6.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "python-3.8.20        | 23.8 MB   | : 100% 1.0/1 [00:00<00:00,  2.96it/s]               \n",
            "pip-24.2             | 2.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.24it/s]\u001b[A\n",
            "pip-24.2             | 2.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.24it/s]\u001b[A\n",
            "\n",
            "setuptools-75.1.0    | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate py38\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Python 3.8.20\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting transformers==4.36.2\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting filelock (from torch==2.0.1)\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions (from torch==2.0.1)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.0.1)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.0.1)\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting jinja2 (from torch==2.0.1)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.36.2)\n",
            "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting packaging>=20.0 (from transformers==4.36.2)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.36.2)\n",
            "  Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.36.2)\n",
            "  Downloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers==4.36.2)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n",
            "  Downloading tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers==4.36.2)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers==4.36.2)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/py38/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/envs/py38/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Collecting cmake (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading cmake-4.0.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2)\n",
            "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.0.1)\n",
            "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers==4.36.2)\n",
            "  Downloading charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers==4.36.2)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.36.2)\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers==4.36.2)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.0.1)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m147.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m155.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m153.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m138.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.5/746.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m137.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Downloading cmake-4.0.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m167.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "Installing collected packages: mpmath, lit, urllib3, typing-extensions, tqdm, sympy, safetensors, regex, pyyaml, packaging, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, cmake, charset-normalizer, certifi, requests, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, huggingface-hub, tokenizers, transformers, triton, torch\n",
            "Successfully installed MarkupSafe-2.1.5 certifi-2025.4.26 charset-normalizer-3.4.2 cmake-4.0.2 filelock-3.16.1 fsspec-2025.3.0 hf-xet-1.1.0 huggingface-hub-0.31.1 idna-3.10 jinja2-3.1.6 lit-18.1.8 mpmath-1.3.0 networkx-3.1 numpy-1.24.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 packaging-25.0 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 sympy-1.13.3 tokenizers-0.15.2 torch-2.0.1 tqdm-4.67.1 transformers-4.36.2 triton-2.0.0 typing-extensions-4.13.2 urllib3-2.2.3\n",
            "Cloning into 'DeepSeek-VL'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 119 (delta 31), reused 15 (delta 15), pack-reused 66 (from 1)\u001b[K\n",
            "Receiving objects: 100% (119/119), 12.47 MiB | 34.97 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n",
            "Obtaining file:///content/DeepSeek-VL\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/envs/py38/lib/python3.8/site-packages (from deepseek_vl==1.0.0) (2.0.1)\n",
            "Collecting transformers>=4.38.2 (from deepseek_vl==1.0.0)\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting timm>=0.9.16 (from deepseek_vl==1.0.0)\n",
            "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
            "Collecting accelerate (from deepseek_vl==1.0.0)\n",
            "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sentencepiece (from deepseek_vl==1.0.0)\n",
            "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting attrdict (from deepseek_vl==1.0.0)\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting einops (from deepseek_vl==1.0.0)\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting torchvision (from timm>=0.9.16->deepseek_vl==1.0.0)\n",
            "  Downloading torchvision-0.19.1-cp38-cp38-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/envs/py38/lib/python3.8/site-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/envs/py38/lib/python3.8/site-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.31.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/envs/py38/lib/python3.8/site-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/envs/py38/lib/python3.8/site-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/py38/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.1->deepseek_vl==1.0.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/envs/py38/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.1->deepseek_vl==1.0.0) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/envs/py38/lib/python3.8/site-packages (from triton==2.0.0->torch>=2.0.1->deepseek_vl==1.0.0) (4.0.2)\n",
            "Requirement already satisfied: lit in /usr/local/envs/py38/lib/python3.8/site-packages (from triton==2.0.0->torch>=2.0.1->deepseek_vl==1.0.0) (18.1.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/envs/py38/lib/python3.8/site-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/envs/py38/lib/python3.8/site-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/envs/py38/lib/python3.8/site-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/envs/py38/lib/python3.8/site-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2.32.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.38.2->deepseek_vl==1.0.0)\n",
            "  Downloading tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/envs/py38/lib/python3.8/site-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (4.67.1)\n",
            "Collecting psutil (from accelerate->deepseek_vl==1.0.0)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting six (from attrdict->deepseek_vl==1.0.0)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/envs/py38/lib/python3.8/site-packages (from huggingface_hub->timm>=0.9.16->deepseek_vl==1.0.0) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/envs/py38/lib/python3.8/site-packages (from huggingface_hub->timm>=0.9.16->deepseek_vl==1.0.0) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/envs/py38/lib/python3.8/site-packages (from jinja2->torch>=2.0.1->deepseek_vl==1.0.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/envs/py38/lib/python3.8/site-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/envs/py38/lib/python3.8/site-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/envs/py38/lib/python3.8/site-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/py38/lib/python3.8/site-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/envs/py38/lib/python3.8/site-packages (from sympy->torch>=2.0.1->deepseek_vl==1.0.0) (1.3.0)\n",
            "Collecting torch>=2.0.1 (from deepseek_vl==1.0.0)\n",
            "  Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->timm>=0.9.16->deepseek_vl==1.0.0)\n",
            "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.1->deepseek_vl==1.0.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
            "Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading torchvision-0.19.1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepseek_vl\n",
            "  Building editable for deepseek_vl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepseek_vl: filename=deepseek_vl-1.0.0-0.editable-py3-none-any.whl size=13124 sha256=bcd964f6c914016a796a18068f8f3eb2de46dac572a81e748c03ade2500ecf31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zui49dio/wheels/73/a4/8a/61d46483981a3ac3d4f78116e149261d2935e02916ed472f1a\n",
            "Successfully built deepseek_vl\n",
            "Installing collected packages: sentencepiece, triton, six, psutil, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, attrdict, tokenizers, nvidia-cusolver-cu12, transformers, torch, torchvision, accelerate, timm, deepseek_vl\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.36.2\n",
            "    Uninstalling transformers-4.36.2:\n",
            "      Successfully uninstalled transformers-4.36.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "Successfully installed accelerate-1.0.1 attrdict-2.0.1 deepseek_vl-1.0.0 einops-0.8.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.41 nvidia-nvtx-cu12-12.1.105 pillow-10.4.0 psutil-7.0.0 sentencepiece-0.2.0 six-1.17.0 timm-1.0.15 tokenizers-0.20.3 torch-2.4.1 torchvision-0.19.1 transformers-4.46.3 triton-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.8/site-packages')\n",
        "\n",
        "!conda create -y -n py38 python=3.8\n",
        "\n",
        "!source /usr/local/bin/activate py38 && python --version\n",
        "\n",
        "!source /usr/local/bin/activate py38 && conda install -y pip\n",
        "!source /usr/local/bin/activate py38 && pip install torch==2.0.1 transformers==4.36.2 numpy==1.24.4\n",
        "\n",
        "!git clone https://github.com/deepseek-ai/DeepSeek-VL.git\n",
        "!source /usr/local/bin/activate py38 && pip install -e ./DeepSeek-VL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "BJN6xF4pwmsz",
        "outputId": "fbfc3ea2-4385-49b1-c075-638819851357"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e86f999a-6085-488e-892c-f52b610a7a42\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e86f999a-6085-488e-892c-f52b610a7a42\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Cytosine.png to Cytosine.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882qZwJ35j9Q",
        "outputId": "ce5e93dd-05d7-4220-ae80-26678efc934d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 149M\n",
            "-rw-r--r-- 1 root root  14K May 11 05:45 Acetic-acid.png\n",
            "-rw-r--r-- 1 root root  26K May 11 05:45 Adenine.png\n",
            "-rw-r--r-- 1 root root  12K May 11 05:45 Ammonia.png\n",
            "-rw-r--r-- 1 root root  30K May 11 05:45 Benzoic_acid.png\n",
            "-rw-r--r-- 1 root root  26K May 11 05:45 Caffeine.png\n",
            "-rw-r--r-- 1 root root  18K May 11 05:51 Cytosine.png\n",
            "drwxr-xr-x 7 root root 4.0K May 11 05:42 DeepSeek-VL\n",
            "-rw-r--r-- 1 root root  17K May 11 05:45 Formic_acid.png\n",
            "-rw-r--r-- 1 root root  21K May 11 05:45 H2O.png\n",
            "-rw-r--r-- 1 root root  15K May 11 05:45 Histamine.png\n",
            "-rw-r--r-- 1 root root  17K May 11 05:45 Ibuprofen.png\n",
            "-rw-r--r-- 1 root root  20K May 11 05:45 Imidazole_full.png\n",
            "-rw-r--r-- 1 root root  12K May 11 05:45 MeNH2.png\n",
            "-rw-r--r-- 1 root root  15K May 11 05:45 Methanol.png\n",
            "-rw-r--r-- 1 root root 149M Apr 30 20:14 Miniconda3-latest-Linux-x86_64.sh\n",
            "-rw-r--r-- 1 root root  25K May 11 05:45 Morphine.png\n",
            "-rw-r--r-- 1 root root  19K May 11 05:45 Nicotinamid.png\n",
            "drwxr-xr-x 2 root root 4.0K May 11 05:47 outputs\n",
            "-rw-r--r-- 1 root root 6.6K May 11 05:45 Phenol.png\n",
            "-rw-r--r-- 1 root root  31K May 11 05:45 Purine.png\n",
            "-rw-r--r-- 1 root root  25K May 11 05:45 Pyridine-full.png\n",
            "-rw-r--r-- 1 root root  28K May 11 05:45 Salicylic-acid.png\n",
            "drwxr-xr-x 1 root root 4.0K May  8 13:38 sample_data\n",
            "-rw-r--r-- 1 root root 3.8K May 11 05:45 task2_deepseek.py\n",
            "-rw-r--r-- 1 root root  25K May 11 05:45 Uracil.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"task2_deepseek.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('''\n",
        "from transformers import AutoTokenizer\n",
        "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
        "from deepseek_vl.utils.io import load_pil_images\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# --- Step 1: Load DeepSeek Model and Processor ---\n",
        "model_id = \"deepseek-ai/deepseek-vl-7b-chat\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = MultiModalityCausalLM.from_pretrained(model_id, trust_remote_code=True).to(torch.bfloat16).cuda().eval()\n",
        "vl_chat_processor = VLChatProcessor.from_pretrained(model_id)\n",
        "tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "# --- Step 2: Set Dataset Folder and Image Pairs ---\n",
        "dataset_folder = \"/content\"\n",
        "\n",
        "image_pairs = [\n",
        "    (\"Benzoic_acid.png\", \"Phenol.png\"),\n",
        "    (\"Formic_acid.png\", \"Acetic-acid.png\"),\n",
        "    (\"Ammonia.png\", \"MeNH2.png\"),\n",
        "    (\"Cytosine.png\", \"Adenine.png\"),\n",
        "    (\"H2O.png\", \"Methanol.png\"),\n",
        "    (\"Caffeine.png\", \"Morphine.png\"),\n",
        "    (\"Ibuprofen.png\", \"Salicylic-acid.png\"),\n",
        "    (\"Imidazole_full.png\", \"Pyridine-full.png\"),\n",
        "    (\"Nicotinamid.png\", \"Histamine.png\"),\n",
        "    (\"Purine.png\", \"Uracil.png\"),\n",
        "]\n",
        "\n",
        "cot_types = [\"Baseline\", \"Stepwise\", \"Visual_first\", \"Explanation_first\"]\n",
        "\n",
        "prompts = {\n",
        "    \"Baseline\": \"Which molecule is a stronger acid or base, the first or the second? Why?\",\n",
        "    \"Stepwise\": \"Step 1: Identify the most acidic or basic site in each molecule. Step 2: Consider how stable the molecule is after gaining or losing a proton. Step 3: Say which molecule is stronger, the first or the second, and explain why.\",\n",
        "    \"Visual_first\": \"Examine the image only. Based on visible atom types and bonds, which molecule appears stronger in acid/base behavior? End by stating 'first' or 'second'.\",\n",
        "    \"Explanation_first\": \"The ability to stabilize charge after proton gain or loss determines acid/base strength. Analyze both molecules, then conclude which is stronger: first or second.\"\n",
        "}\n",
        "\n",
        "# --- Step 3: Output Setup ---\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "csv_file = open(\"outputs/Task2_deepseek_outputs.csv\", \"w\", encoding=\"utf-8\", newline=\"\")\n",
        "writer = csv.DictWriter(csv_file, fieldnames=[\"image1\", \"image2\", \"prompt_type\", \"prompt_text\", \"generated_text\"])\n",
        "writer.writeheader()\n",
        "\n",
        "# --- Step 4: Run Inference ---\n",
        "for img1, img2 in image_pairs:\n",
        "    img_path1 = os.path.join(dataset_folder, img1)\n",
        "    img_path2 = os.path.join(dataset_folder, img2)\n",
        "    for cot in cot_types:\n",
        "        prompt = prompts[cot]\n",
        "        conversation = [\n",
        "            {\n",
        "                \"role\": \"User\",\n",
        "                \"content\": prompt,\n",
        "                \"images\": [img_path1, img_path2]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"Assistant\",\n",
        "                \"content\": \"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        images = load_pil_images(conversation)\n",
        "        prepare_inputs = vl_chat_processor(\n",
        "            conversations=conversation,\n",
        "            images=images,\n",
        "            force_batchify=True\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = model.prepare_inputs_embeds(**prepare_inputs)\n",
        "            outputs = model.language_model.generate(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                attention_mask=prepare_inputs.attention_mask,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                bos_token_id=tokenizer.bos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=False,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "        print(f\"--- Finished {cot} for {img1} and {img2}\")\n",
        "        writer.writerow({\n",
        "            \"image1\": img1,\n",
        "            \"image2\": img2,\n",
        "            \"prompt_type\": cot,\n",
        "            \"prompt_text\": prompt,\n",
        "            \"generated_text\": answer.strip()\n",
        "        })\n",
        "\n",
        "csv_file.close()\n",
        "print(\"!!!! All results saved to outputs/Task2_deepseek_outputs.csv\")\n",
        "''')\n"
      ],
      "metadata": {
        "id": "CIZ8ndHc0ptR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /usr/local/bin/activate py38 && python task2_deepseek.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8VfZ7I-wpso",
        "outputId": "9582fb9b-a612-40ca-84ad-966956a45ca0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/envs/py38/lib/python3.8/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
            "  warnings.warn(\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.51s/it]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
            "Some kwargs in processor config are unused and will not have any effect: mask_prompt, sft_format, num_image_tokens, add_special_token, ignore_id, image_tag. \n",
            "--- Finished Baseline for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Stepwise for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Visual_first for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Explanation_first for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Baseline for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Stepwise for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Visual_first for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Explanation_first for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Baseline for Ammonia.png and MeNH2.png\n",
            "--- Finished Stepwise for Ammonia.png and MeNH2.png\n",
            "--- Finished Visual_first for Ammonia.png and MeNH2.png\n",
            "--- Finished Explanation_first for Ammonia.png and MeNH2.png\n",
            "--- Finished Baseline for Cytosine.png and Adenine.png\n",
            "--- Finished Stepwise for Cytosine.png and Adenine.png\n",
            "--- Finished Visual_first for Cytosine.png and Adenine.png\n",
            "--- Finished Explanation_first for Cytosine.png and Adenine.png\n",
            "--- Finished Baseline for H2O.png and Methanol.png\n",
            "--- Finished Stepwise for H2O.png and Methanol.png\n",
            "--- Finished Visual_first for H2O.png and Methanol.png\n",
            "--- Finished Explanation_first for H2O.png and Methanol.png\n",
            "--- Finished Baseline for Caffeine.png and Morphine.png\n",
            "--- Finished Stepwise for Caffeine.png and Morphine.png\n",
            "--- Finished Visual_first for Caffeine.png and Morphine.png\n",
            "--- Finished Explanation_first for Caffeine.png and Morphine.png\n",
            "--- Finished Baseline for Ibuprofen.png and Salicylic-acid.png\n",
            "--- Finished Stepwise for Ibuprofen.png and Salicylic-acid.png\n",
            "--- Finished Visual_first for Ibuprofen.png and Salicylic-acid.png\n",
            "--- Finished Explanation_first for Ibuprofen.png and Salicylic-acid.png\n",
            "--- Finished Baseline for Imidazole_full.png and Pyridine-full.png\n",
            "--- Finished Stepwise for Imidazole_full.png and Pyridine-full.png\n",
            "--- Finished Visual_first for Imidazole_full.png and Pyridine-full.png\n",
            "--- Finished Explanation_first for Imidazole_full.png and Pyridine-full.png\n",
            "--- Finished Baseline for Nicotinamid.png and Histamine.png\n",
            "--- Finished Stepwise for Nicotinamid.png and Histamine.png\n",
            "--- Finished Visual_first for Nicotinamid.png and Histamine.png\n",
            "--- Finished Explanation_first for Nicotinamid.png and Histamine.png\n",
            "--- Finished Baseline for Purine.png and Uracil.png\n",
            "--- Finished Stepwise for Purine.png and Uracil.png\n",
            "--- Finished Visual_first for Purine.png and Uracil.png\n",
            "--- Finished Explanation_first for Purine.png and Uracil.png\n",
            "!!!! All results saved to outputs/Task2_deepseek_outputs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/outputs/Task2_deepseek_outputs.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xatIllqNxNBO",
        "outputId": "4aa4e2d2-5938-4256-afd2-ef76e5c42c4c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d9b83b66-1599-4a4f-8d7a-af5015d04e0b\", \"Task2_deepseek_outputs.csv\", 42948)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rx4gkWdJxM-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"task2-deepseek-finetune.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\"\"\n",
        "from transformers import AutoTokenizer\n",
        "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
        "from deepseek_vl.utils.io import load_pil_images\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# ----- 1. Setup Model -----\n",
        "model_id = \"deepseek-ai/deepseek-vl-7b-chat\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = MultiModalityCausalLM.from_pretrained(model_id, trust_remote_code=True).to(torch.bfloat16).cuda().eval()\n",
        "vl_chat_processor = VLChatProcessor.from_pretrained(model_id)\n",
        "tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "# ----- 2. Define Prompts (Baseline + 15 CoT prompts) -----\n",
        "baseline_prompt = [\"Which molecule is a stronger acid or base, the first or the second? Why?\"]\n",
        "\n",
        "stepwise_prompts = [\n",
        "    \"Step 1: Identify the most acidic or basic site in each molecule. Step 2: Consider how stable the molecule is after gaining or losing a proton. Step 3: Say which molecule is stronger, the first or the second, and explain why.\",\n",
        "    \"Step 1: Find the most likely proton donor or acceptor in each molecule. Step 2: Compare their strength based on electronegativity or charge stability. Step 3: Pick the more reactive one.\"\n",
        "]\n",
        "\n",
        "visual_first_prompts = [\n",
        "    \"Examine the image only. Based on visible atom types and bonds, which molecule appears stronger in acid/base behavior? End by stating 'first' or 'second'.\",\n",
        "    \"Look at the molecular structures and visually compare atoms and groups. Which molecule has more acidic or basic features visible in the image? Choose first or second.\"\n",
        "]\n",
        "\n",
        "explanation_first_prompts = [\n",
        "    \"The ability to stabilize charge after proton gain or loss determines acid/base strength. Analyze both molecules, then conclude which is stronger: first or second.\",\n",
        "    \"Stronger acids form more stable conjugate bases. Based on that, explain which molecule is more acidic, and say if it’s the first or second molecule.\"\n",
        "]\n",
        "\n",
        "all_prompts = baseline_prompt + stepwise_prompts + visual_first_prompts + explanation_first_prompts\n",
        "\n",
        "cot_types = (\n",
        "    [\"Baseline\"] +\n",
        "    [\"Stepwise\"] * len(stepwise_prompts) +\n",
        "    [\"Visual_first\"] * len(visual_first_prompts) +\n",
        "    [\"Explanation_first\"] * len(explanation_first_prompts)\n",
        ")\n",
        "\n",
        "# ----- 3. Define Image Pairs -----\n",
        "dataset_folder = \"/content\"\n",
        "image_pairs = [\n",
        "    (\"Benzoic_acid.png\", \"Phenol.png\"),\n",
        "    (\"Formic_acid.png\", \"Acetic-acid.png\"),\n",
        "    (\"Ammonia.png\", \"MeNH2.png\")\n",
        "]\n",
        "\n",
        "# ----- 4. Output Setup -----\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "csv_file = open(\"outputs/Task2_deepseek_finetune-res.csv\", \"w\", encoding=\"utf-8\", newline=\"\")\n",
        "writer = csv.DictWriter(csv_file, fieldnames=[\"image1\", \"image2\", \"prompt_type\", \"prompt_number\", \"prompt_text\", \"generated_text\"])\n",
        "writer.writeheader()\n",
        "\n",
        "# ----- 5. Inference -----\n",
        "for img1, img2 in image_pairs:\n",
        "    img_path1 = os.path.join(dataset_folder, img1)\n",
        "    img_path2 = os.path.join(dataset_folder, img2)\n",
        "    for idx, prompt_template in enumerate(all_prompts):\n",
        "        prompt_type = cot_types[idx]\n",
        "        conversation = [\n",
        "            {\n",
        "                \"role\": \"User\",\n",
        "                \"content\": prompt_template,\n",
        "                \"images\": [img_path1, img_path2]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"Assistant\",\n",
        "                \"content\": \"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        images = load_pil_images(conversation)\n",
        "        prepare_inputs = vl_chat_processor(\n",
        "            conversations=conversation,\n",
        "            images=images,\n",
        "            force_batchify=True\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = model.prepare_inputs_embeds(**prepare_inputs)\n",
        "            outputs = model.language_model.generate(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                attention_mask=prepare_inputs.attention_mask,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                bos_token_id=tokenizer.bos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=False,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "        print(f\"--- Finished {prompt_type} #{idx+1} for {img1} and {img2}\")\n",
        "        writer.writerow({\n",
        "            \"image1\": img1,\n",
        "            \"image2\": img2,\n",
        "            \"prompt_type\": prompt_type,\n",
        "            \"prompt_number\": idx + 1,\n",
        "            \"prompt_text\": prompt_template,\n",
        "            \"generated_text\": answer.strip()\n",
        "        })\n",
        "\n",
        "csv_file.close()\n",
        "print(\"!!!! All pair inference completed and saved to outputs/Task2_deepseek_finetune-res.csv\")\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "5WeKMxPQ6d0k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /usr/local/bin/activate py38 && python task2-deepseek-finetune.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5G9TdiX7902",
        "outputId": "ba8d01a7-cb6a-4185-ca1f-f95f4a26daec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/envs/py38/lib/python3.8/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
            "  warnings.warn(\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.36s/it]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
            "Some kwargs in processor config are unused and will not have any effect: add_special_token, sft_format, mask_prompt, image_tag, num_image_tokens, ignore_id. \n",
            "--- Finished Baseline #1 for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Stepwise #2 for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Stepwise #3 for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Visual_first #4 for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Visual_first #5 for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Explanation_first #6 for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Explanation_first #7 for Benzoic_acid.png and Phenol.png\n",
            "--- Finished Baseline #1 for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Stepwise #2 for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Stepwise #3 for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Visual_first #4 for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Visual_first #5 for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Explanation_first #6 for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Explanation_first #7 for Formic_acid.png and Acetic-acid.png\n",
            "--- Finished Baseline #1 for Ammonia.png and MeNH2.png\n",
            "--- Finished Stepwise #2 for Ammonia.png and MeNH2.png\n",
            "--- Finished Stepwise #3 for Ammonia.png and MeNH2.png\n",
            "--- Finished Visual_first #4 for Ammonia.png and MeNH2.png\n",
            "--- Finished Visual_first #5 for Ammonia.png and MeNH2.png\n",
            "--- Finished Explanation_first #6 for Ammonia.png and MeNH2.png\n",
            "--- Finished Explanation_first #7 for Ammonia.png and MeNH2.png\n",
            "!!!! All pair inference completed and saved to outputs/Task2_deepseek_finetune-res.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/outputs/Task2_deepseek_finetune-res.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MaTmuK0b8Bgt",
        "outputId": "c3fa6a1f-b219-4fa7-e027-b39242bea948"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_76b98882-8b24-43f7-9073-21698000215c\", \"Task2_deepseek_finetune-res.csv\", 21315)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}